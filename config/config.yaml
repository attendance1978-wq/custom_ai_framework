model:
  name: gpt2
  vocab_size: 50257
  n_positions: 1024
  n_embd: 768
  n_layer: 12
  n_head: 12

training:
  num_epochs: 3
  batch_size: 4
  learning_rate: 5e-5
  save_steps: 500
  max_length: 512

data:
  raw_path: data/raw/dialogues.jsonl
  processed_path: data/processed/train.jsonl

paths:
  models_base: models/base
  models_fine_tuned: models/fine_tuned
  tokenizer: models/tokenizer
  logs: logs
